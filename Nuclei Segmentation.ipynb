{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb921efe-b24e-4ef3-8015-edab810fc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, measure, filters\n",
    "from cellpose import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1219b5-318b-499f-96eb-cc4d200c3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NucleiSegmentationPipeline:\n",
    "    \"\"\"\n",
    "    Automated nuclei segmentation and phenotyping pipeline (low VRAM & CPU fallback).\n",
    "    Optimized for batch dataset processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir='results'):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        (self.output_dir / 'plots').mkdir(exist_ok=True)\n",
    "\n",
    "        self.results = []\n",
    "        self.all_features = []  # To accumulate feature DataFrames across images\n",
    "\n",
    "        # Use GPU if available\n",
    "        self.model = models.CellposeModel(gpu=True)\n",
    "        print(\"‚úÖ CellPose model loaded (GPU mode)\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Image preprocessing\n",
    "    # ------------------------------------------------------------------------\n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Normalize and enhance image quality for CellPose\"\"\"\n",
    "        # Handle RGBA or RGB input\n",
    "        if len(image.shape) == 3:\n",
    "            # Drop alpha if present\n",
    "            if image.shape[2] == 4:\n",
    "                image = image[:, :, :3]\n",
    "            # Convert to grayscale by averaging\n",
    "            image = np.mean(image, axis=2)\n",
    "    \n",
    "        image = image.astype(np.float32)\n",
    "        image = (image - image.min()) / (image.max() - image.min() + 1e-8)\n",
    "        image = filters.gaussian(image, sigma=1.0)\n",
    "        return image\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Segmentation\n",
    "    # ------------------------------------------------------------------------\n",
    "    def segment_cellpose(self, image):\n",
    "        \"\"\"Run CellPose segmentation (compatible with v3.x and v4.x)\"\"\"\n",
    "        image_uint8 = (image * 255).astype(np.uint8)\n",
    "        \n",
    "        try:\n",
    "            outputs = self.model.eval(\n",
    "                image_uint8,\n",
    "                diameter=30,\n",
    "                channels=[0, 0]\n",
    "            )\n",
    "            \n",
    "            # Handle both 3- and 4-return signatures\n",
    "            if len(outputs) == 4:\n",
    "                masks, flows, styles, diams = outputs\n",
    "            elif len(outputs) == 3:\n",
    "                masks, flows, styles = outputs\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected number of return values from CellPose: {len(outputs)}\")\n",
    "            \n",
    "            return masks\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in CellPose segmentation: {e}\")\n",
    "            return np.zeros_like(image_uint8)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Feature extraction\n",
    "    # ------------------------------------------------------------------------\n",
    "    def extract_features(self, image, labels, image_name):\n",
    "        \"\"\"Extract per-nucleus features and tag with image name\"\"\"\n",
    "        props = measure.regionprops_table(\n",
    "            labels,\n",
    "            intensity_image=image,\n",
    "            properties=[\n",
    "                'label', 'area', 'perimeter', 'eccentricity',\n",
    "                'solidity', 'intensity_mean', 'intensity_max',\n",
    "                'centroid', 'major_axis_length', 'minor_axis_length'\n",
    "            ]\n",
    "        )\n",
    "        df = pd.DataFrame(props)\n",
    "        df['circularity'] = (4 * np.pi * df['area']) / (df['perimeter'] ** 2 + 1e-8)\n",
    "        df['aspect_ratio'] = df['major_axis_length'] / (df['minor_axis_length'] + 1e-8)\n",
    "        df['image_name'] = image_name\n",
    "        return df\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Population metrics\n",
    "    # ------------------------------------------------------------------------\n",
    "    def calculate_metrics(self, labels):\n",
    "        regions = measure.regionprops(labels)\n",
    "        if not regions:\n",
    "            return {'num_nuclei': 0, 'mean_area': 0, 'std_area': 0, 'density': 0}\n",
    "\n",
    "        num_nuclei = len(regions)\n",
    "        areas = [r.area for r in regions]\n",
    "        metrics = {\n",
    "            'num_nuclei': num_nuclei,\n",
    "            'mean_area': np.mean(areas),\n",
    "            'std_area': np.std(areas),\n",
    "            'density': num_nuclei / (labels.shape[0] * labels.shape[1]) * 1e6  # per mm¬≤\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Visualization\n",
    "    # ------------------------------------------------------------------------\n",
    "    def visualize_results(self, image, labels, features_df, save_path):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "        # Original\n",
    "        axes[0, 0].imshow(image, cmap='gray')\n",
    "        axes[0, 0].set_title('Original')\n",
    "        axes[0, 0].axis('off')\n",
    "\n",
    "        # Segmentation overlay\n",
    "        axes[0, 1].imshow(image, cmap='gray')\n",
    "        axes[0, 1].imshow(labels, cmap='tab20', alpha=0.5)\n",
    "        axes[0, 1].set_title(f'Segmentation ({len(features_df)} nuclei)')\n",
    "        axes[0, 1].axis('off')\n",
    "\n",
    "        # Labeled\n",
    "        axes[0, 2].imshow(labels, cmap='nipy_spectral')\n",
    "        axes[0, 2].set_title('Labeled Regions')\n",
    "        axes[0, 2].axis('off')\n",
    "\n",
    "        # Area distribution\n",
    "        axes[1, 0].hist(features_df['area'], bins=30, edgecolor='black')\n",
    "        axes[1, 0].set_title('Area Distribution')\n",
    "        axes[1, 0].set_xlabel('Area (px¬≤)')\n",
    "\n",
    "        # Circularity distribution\n",
    "        axes[1, 1].hist(features_df['circularity'], bins=30, edgecolor='black')\n",
    "        axes[1, 1].set_title('Circularity Distribution')\n",
    "\n",
    "        # Scatter\n",
    "        axes[1, 2].scatter(features_df['area'], features_df['intensity_mean'], alpha=0.6, s=15)\n",
    "        axes[1, 2].set_xlabel('Area')\n",
    "        axes[1, 2].set_ylabel('Mean Intensity')\n",
    "        axes[1, 2].set_title('Area vs Intensity')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Image processing entry point\n",
    "    # ------------------------------------------------------------------------\n",
    "    def process_image(self, image_path, save_results=True):\n",
    "        image = io.imread(image_path)\n",
    "        image = self.preprocess_image(image)\n",
    "        labels = self.segment_cellpose(image)\n",
    "\n",
    "        image_name = Path(image_path).stem\n",
    "        features = self.extract_features(image, labels, image_name)\n",
    "        metrics = self.calculate_metrics(labels)\n",
    "\n",
    "        if save_results:\n",
    "            plot_path = self.output_dir / 'plots' / f'{image_name}_analysis.png'\n",
    "            mask_path = self.output_dir / 'plots' / f'{image_name}_mask.tif'\n",
    "\n",
    "            self.visualize_results(image, labels, features, plot_path)\n",
    "            io.imsave(mask_path, labels.astype(np.uint16))\n",
    "\n",
    "        # Accumulate results\n",
    "        self.results.append({\n",
    "            'image': image_name,\n",
    "            **metrics\n",
    "        })\n",
    "        self.all_features.append(features)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Dataset processing (recursive search)\n",
    "    # ------------------------------------------------------------------------\n",
    "    def process_dataset(self, image_dir, pattern='*.tif'):\n",
    "        \"\"\"Recursively scan directories for images and process all.\"\"\"\n",
    "        image_paths = list(Path(image_dir).rglob(pattern))\n",
    "        if not image_paths:\n",
    "            print(f\"No images found matching {pattern} in {image_dir}\")\n",
    "            return\n",
    "\n",
    "        print(f\"üìÅ Found {len(image_paths)} images under {image_dir}\")\n",
    "        cnt = 0\n",
    "\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                self.process_image(str(img_path))\n",
    "                cnt += 1\n",
    "                print(f\"{cnt} out of {len(image_paths)} is successfully loaded.\", end='\\r', flush=True)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {img_path}: {e}\")\n",
    "\n",
    "        # Save aggregated results\n",
    "        print()\n",
    "        self.save_combined_results()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Aggregated results saving\n",
    "    # ------------------------------------------------------------------------\n",
    "def save_combined_results(self, drop_exact_duplicates=False):\n",
    "    # Columns you want to keep\n",
    "    columns = ['image_name','area','perimeter','eccentricity','solidity',\n",
    "               'intensity_mean','intensity_max','centroid-0','centroid-1',\n",
    "               'major_axis_length','minor_axis_length','circularity','aspect_ratio']\n",
    "    \n",
    "    # Concatenate all DataFrames and keep only the desired columns\n",
    "    df = pd.concat([df_image[columns] for df_image in pipeline.all_features], ignore_index=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('features.csv', index=False)\n",
    "    \n",
    "    print(\"CSV saved as features.csv\")\n",
    "\n",
    "    print(f\"‚úÖ Created new combined CSV at: {out_path} ({len(new_df)} rows)\")\n",
    "\n",
    "    # Save summary and dataset plots even if combined_df is empty\n",
    "    summary = {\n",
    "        'total_images': len(self.results),\n",
    "        'total_nuclei': sum(r['num_nuclei'] for r in self.results),\n",
    "        'mean_nuclei_per_image': float(np.mean([r['num_nuclei'] for r in self.results])) if self.results else 0.0,\n",
    "        'mean_nucleus_area': float(np.mean([r['mean_area'] for r in self.results])) if self.results else 0.0,\n",
    "        'processing_date': datetime.now().isoformat()\n",
    "    }\n",
    "    with open(self.output_dir / 'summary_report.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    self.plot_dataset_summary()\n",
    "    print(\"üßæ Summary report saved.\")\n",
    "\n",
    "    return \n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Dataset-level summary visualization\n",
    "    # ------------------------------------------------------------------------\n",
    "    def plot_dataset_summary(self):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "        counts = [r['num_nuclei'] for r in self.results]\n",
    "        axes[0].bar(range(len(counts)), counts)\n",
    "        axes[0].set_title('Nuclei per Image')\n",
    "        axes[0].set_xlabel('Image Index')\n",
    "        axes[0].set_ylabel('Count')\n",
    "\n",
    "        areas = [r['mean_area'] for r in self.results]\n",
    "        axes[1].hist(areas, bins=20, edgecolor='black')\n",
    "        axes[1].set_title('Mean Nucleus Area Distribution')\n",
    "        axes[1].set_xlabel('Area')\n",
    "\n",
    "        densities = [r['density'] for r in self.results]\n",
    "        axes[2].plot(densities, marker='o')\n",
    "        axes[2].set_title('Cell Density Variation')\n",
    "        axes[2].set_xlabel('Image Index')\n",
    "        axes[2].set_ylabel('Density (nuclei/mm¬≤)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'dataset_summary.png', dpi=150)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6a2dd-7289-4243-a850-426fd3b587e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process single image\n",
    "pipeline = NucleiSegmentationPipeline()\n",
    "\n",
    "# For BBBC038 dataset:\n",
    "pipeline.process_dataset('/home/amon/Cell-Segmentation-Morphology/dataset', pattern='*/images/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e45f91c0-f3f6-4d25-b2cc-09dcf3a8e86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved as features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Columns you want to keep\n",
    "columns = ['image_name','area','perimeter','eccentricity','solidity',\n",
    "           'intensity_mean','intensity_max','centroid-0','centroid-1',\n",
    "           'major_axis_length','minor_axis_length','circularity','aspect_ratio']\n",
    "\n",
    "# Concatenate all DataFrames and keep only the desired columns\n",
    "df = pd.concat([df_image[columns] for df_image in pipeline.all_features], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('features.csv', index=False)\n",
    "\n",
    "print(\"CSV saved as features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d34f5630-b03e-41e2-8991-14e57c503e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>solidity</th>\n",
       "      <th>intensity_mean</th>\n",
       "      <th>intensity_max</th>\n",
       "      <th>centroid-0</th>\n",
       "      <th>centroid-1</th>\n",
       "      <th>major_axis_length</th>\n",
       "      <th>minor_axis_length</th>\n",
       "      <th>circularity</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>41.142136</td>\n",
       "      <td>0.856954</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.311882</td>\n",
       "      <td>0.443204</td>\n",
       "      <td>3.263636</td>\n",
       "      <td>205.181818</td>\n",
       "      <td>16.662239</td>\n",
       "      <td>8.587606</td>\n",
       "      <td>0.816637</td>\n",
       "      <td>1.940266</td>\n",
       "      <td>fc5452f612a0f972fe55cc677055ede662af6723b5c161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>187.0</td>\n",
       "      <td>48.627417</td>\n",
       "      <td>0.608791</td>\n",
       "      <td>0.973958</td>\n",
       "      <td>0.202061</td>\n",
       "      <td>0.291210</td>\n",
       "      <td>8.427807</td>\n",
       "      <td>153.438503</td>\n",
       "      <td>17.333885</td>\n",
       "      <td>13.751500</td>\n",
       "      <td>0.993777</td>\n",
       "      <td>1.260509</td>\n",
       "      <td>fc5452f612a0f972fe55cc677055ede662af6723b5c161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>268.0</td>\n",
       "      <td>59.698485</td>\n",
       "      <td>0.447635</td>\n",
       "      <td>0.950355</td>\n",
       "      <td>0.291927</td>\n",
       "      <td>0.426615</td>\n",
       "      <td>15.145522</td>\n",
       "      <td>208.884328</td>\n",
       "      <td>19.609088</td>\n",
       "      <td>17.534767</td>\n",
       "      <td>0.944970</td>\n",
       "      <td>1.118298</td>\n",
       "      <td>fc5452f612a0f972fe55cc677055ede662af6723b5c161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>183.0</td>\n",
       "      <td>48.384776</td>\n",
       "      <td>0.630578</td>\n",
       "      <td>0.973404</td>\n",
       "      <td>0.337899</td>\n",
       "      <td>0.618392</td>\n",
       "      <td>14.825137</td>\n",
       "      <td>249.491803</td>\n",
       "      <td>17.436687</td>\n",
       "      <td>13.533063</td>\n",
       "      <td>0.982298</td>\n",
       "      <td>1.288451</td>\n",
       "      <td>fc5452f612a0f972fe55cc677055ede662af6723b5c161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>303.0</td>\n",
       "      <td>63.112698</td>\n",
       "      <td>0.436021</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.325888</td>\n",
       "      <td>0.528196</td>\n",
       "      <td>23.745875</td>\n",
       "      <td>116.792079</td>\n",
       "      <td>20.714770</td>\n",
       "      <td>18.641974</td>\n",
       "      <td>0.955914</td>\n",
       "      <td>1.111190</td>\n",
       "      <td>fc5452f612a0f972fe55cc677055ede662af6723b5c161...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   area  perimeter  eccentricity  solidity  intensity_mean  \\\n",
       "0      1  110.0  41.142136      0.856954  0.916667        0.311882   \n",
       "1      2  187.0  48.627417      0.608791  0.973958        0.202061   \n",
       "2      3  268.0  59.698485      0.447635  0.950355        0.291927   \n",
       "3      4  183.0  48.384776      0.630578  0.973404        0.337899   \n",
       "4      5  303.0  63.112698      0.436021  0.961905        0.325888   \n",
       "\n",
       "   intensity_max  centroid-0  centroid-1  major_axis_length  \\\n",
       "0       0.443204    3.263636  205.181818          16.662239   \n",
       "1       0.291210    8.427807  153.438503          17.333885   \n",
       "2       0.426615   15.145522  208.884328          19.609088   \n",
       "3       0.618392   14.825137  249.491803          17.436687   \n",
       "4       0.528196   23.745875  116.792079          20.714770   \n",
       "\n",
       "   minor_axis_length  circularity  aspect_ratio  \\\n",
       "0           8.587606     0.816637      1.940266   \n",
       "1          13.751500     0.993777      1.260509   \n",
       "2          17.534767     0.944970      1.118298   \n",
       "3          13.533063     0.982298      1.288451   \n",
       "4          18.641974     0.955914      1.111190   \n",
       "\n",
       "                                          image_name  \n",
       "0  fc5452f612a0f972fe55cc677055ede662af6723b5c161...  \n",
       "1  fc5452f612a0f972fe55cc677055ede662af6723b5c161...  \n",
       "2  fc5452f612a0f972fe55cc677055ede662af6723b5c161...  \n",
       "3  fc5452f612a0f972fe55cc677055ede662af6723b5c161...  \n",
       "4  fc5452f612a0f972fe55cc677055ede662af6723b5c161...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.all_features[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acdcee7-2d20-48a7-a956-dfea8dba3810",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ace73b-330f-4b0f-b0cb-a023f7c01806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate segmentation performance against ground truth masks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir='results/evaluation'):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.evaluation_results = []\n",
    "    \n",
    "    def calculate_iou(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate Intersection over Union (IoU) for binary masks\n",
    "        \n",
    "        Args:\n",
    "            pred_mask: Predicted segmentation mask\n",
    "            true_mask: Ground truth mask\n",
    "            \n",
    "        Returns:\n",
    "            IoU score (0-1)\n",
    "        \"\"\"\n",
    "        # Binarize masks\n",
    "        pred_binary = (pred_mask > 0).astype(np.uint8)\n",
    "        true_binary = (true_mask > 0).astype(np.uint8)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.logical_and(pred_binary, true_binary).sum()\n",
    "        union = np.logical_or(pred_binary, true_binary).sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        iou = intersection / union\n",
    "        return float(iou)\n",
    "    \n",
    "    def calculate_dice(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate Dice coefficient (F1 score for segmentation)\n",
    "        \"\"\"\n",
    "        pred_binary = (pred_mask > 0).astype(np.uint8)\n",
    "        true_binary = (true_mask > 0).astype(np.uint8)\n",
    "        \n",
    "        intersection = np.logical_and(pred_binary, true_binary).sum()\n",
    "        \n",
    "        if pred_binary.sum() + true_binary.sum() == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        dice = 2 * intersection / (pred_binary.sum() + true_binary.sum())\n",
    "        return float(dice)\n",
    "    \n",
    "    def calculate_pixel_accuracy(self, pred_mask, true_mask):\n",
    "        \"\"\"Calculate pixel-wise accuracy\"\"\"\n",
    "        pred_binary = (pred_mask > 0).astype(np.uint8)\n",
    "        true_binary = (true_mask > 0).astype(np.uint8)\n",
    "        \n",
    "        correct = (pred_binary == true_binary).sum()\n",
    "        total = pred_binary.size\n",
    "        \n",
    "        return float(correct / total)\n",
    "    \n",
    "    def calculate_object_level_metrics(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate object-level detection metrics\n",
    "        (True Positives, False Positives, False Negatives at nucleus level)\n",
    "        \"\"\"\n",
    "        from skimage import measure\n",
    "        \n",
    "        # Get connected components\n",
    "        pred_labels = measure.label(pred_mask > 0)\n",
    "        true_labels = measure.label(true_mask > 0)\n",
    "        \n",
    "        n_pred = pred_labels.max()\n",
    "        n_true = true_labels.max()\n",
    "        \n",
    "        # Simple matching: a predicted object is TP if it overlaps with true object\n",
    "        matched_pred = set()\n",
    "        matched_true = set()\n",
    "        \n",
    "        for i in range(1, n_pred + 1):\n",
    "            pred_region = (pred_labels == i)\n",
    "            overlapping_true = np.unique(true_labels[pred_region])\n",
    "            overlapping_true = overlapping_true[overlapping_true > 0]\n",
    "            \n",
    "            if len(overlapping_true) > 0:\n",
    "                matched_pred.add(i)\n",
    "                matched_true.update(overlapping_true)\n",
    "        \n",
    "        tp = len(matched_pred)\n",
    "        fp = n_pred - tp\n",
    "        fn = n_true - len(matched_true)\n",
    "        \n",
    "        # Calculate precision, recall, F1\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'n_predicted': n_pred,\n",
    "            'n_true': n_true,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    def evaluate_single_image(self, pred_mask_path, true_mask_path, image_name=None):\n",
    "        \"\"\"\n",
    "        Evaluate a single prediction against ground truth\n",
    "        \"\"\"\n",
    "        if image_name is None:\n",
    "            image_name = Path(pred_mask_path).stem\n",
    "        \n",
    "        # Load masks\n",
    "        pred_mask = io.imread(pred_mask_path)\n",
    "        true_mask = io.imread(true_mask_path)\n",
    "        \n",
    "        # Handle RGB masks (take first channel)\n",
    "        if len(true_mask.shape) == 3:\n",
    "            true_mask = true_mask[:, :, 0]\n",
    "        if len(pred_mask.shape) == 3:\n",
    "            pred_mask = pred_mask[:, :, 0]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        iou = self.calculate_iou(pred_mask, true_mask)\n",
    "        dice = self.calculate_dice(pred_mask, true_mask)\n",
    "        pixel_acc = self.calculate_pixel_accuracy(pred_mask, true_mask)\n",
    "        object_metrics = self.calculate_object_level_metrics(pred_mask, true_mask)\n",
    "        \n",
    "        result = {\n",
    "            'image_name': image_name,\n",
    "            'iou': iou,\n",
    "            'dice': dice,\n",
    "            'pixel_accuracy': pixel_acc,\n",
    "            **object_metrics\n",
    "        }\n",
    "        \n",
    "        self.evaluation_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def evaluate_dataset(self, pred_mask_dir, true_mask_dir, pattern='*.png'):\n",
    "        \"\"\"\n",
    "        Evaluate all predictions in a directory against ground truth\n",
    "        \n",
    "        Args:\n",
    "            pred_mask_dir: Directory with predicted masks (e.g., 'results/plots')\n",
    "            true_mask_dir: Root directory with ground truth (e.g., 'dataset')\n",
    "        \"\"\"\n",
    "        print(\"üîç Starting evaluation against ground truth...\")\n",
    "        \n",
    "        # Find all predicted masks\n",
    "        pred_mask_paths = list(Path(pred_mask_dir).rglob('*_mask.tif'))\n",
    "        print(f\"Found {len(pred_mask_paths)} predicted masks\")\n",
    "        \n",
    "        # Match with ground truth\n",
    "        evaluated = 0\n",
    "        skipped = 0\n",
    "        \n",
    "        for pred_path in pred_mask_paths:\n",
    "            # Extract image ID (remove '_mask.tif')\n",
    "            image_id = pred_path.stem.replace('_mask', '')\n",
    "            \n",
    "            # Find corresponding ground truth masks\n",
    "            # BBBC038 structure: dataset/{image_id}/masks/*.png\n",
    "            true_mask_dir_path = Path(true_mask_dir)\n",
    "            true_mask_files = list(true_mask_dir_path.glob(f'{image_id}/masks/*.png'))\n",
    "            \n",
    "            if not true_mask_files:\n",
    "                # Try without subdirectory (in case structure is different)\n",
    "                true_mask_files = list(true_mask_dir_path.glob(f'*/masks/{image_id}*.png'))\n",
    "            \n",
    "            if not true_mask_files:\n",
    "                print(f\"‚ö†Ô∏è  No ground truth found for {image_id}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # If multiple masks, combine them (BBBC038 has one mask per nucleus)\n",
    "            if len(true_mask_files) > 1:\n",
    "                # Combine all individual masks into one\n",
    "                combined_mask = None\n",
    "                for mask_file in true_mask_files:\n",
    "                    mask = io.imread(mask_file)\n",
    "                    if len(mask.shape) == 3:\n",
    "                        mask = mask[:, :, 0]\n",
    "                    if combined_mask is None:\n",
    "                        combined_mask = np.zeros_like(mask, dtype=np.uint16)\n",
    "                    combined_mask = np.maximum(combined_mask, (mask > 0).astype(np.uint16))\n",
    "                true_mask_path = combined_mask\n",
    "            else:\n",
    "                true_mask_path = true_mask_files[0]\n",
    "            \n",
    "            # Evaluate\n",
    "            try:\n",
    "                if isinstance(true_mask_path, np.ndarray):\n",
    "                    # Handle combined mask\n",
    "                    pred_mask = io.imread(pred_path)\n",
    "                    if len(pred_mask.shape) == 3:\n",
    "                        pred_mask = pred_mask[:, :, 0]\n",
    "                    \n",
    "                    iou = self.calculate_iou(pred_mask, true_mask_path)\n",
    "                    dice = self.calculate_dice(pred_mask, true_mask_path)\n",
    "                    pixel_acc = self.calculate_pixel_accuracy(pred_mask, true_mask_path)\n",
    "                    object_metrics = self.calculate_object_level_metrics(pred_mask, true_mask_path)\n",
    "                    \n",
    "                    result = {\n",
    "                        'image_name': image_id,\n",
    "                        'iou': iou,\n",
    "                        'dice': dice,\n",
    "                        'pixel_accuracy': pixel_acc,\n",
    "                        **object_metrics\n",
    "                    }\n",
    "                    self.evaluation_results.append(result)\n",
    "                else:\n",
    "                    self.evaluate_single_image(pred_path, true_mask_path, image_id)\n",
    "                \n",
    "                evaluated += 1\n",
    "                print(f\"‚úì Evaluated {evaluated}/{len(pred_mask_paths)}\", end='\\r', flush=True)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error evaluating {image_id}: {e}\")\n",
    "                skipped += 1\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation complete: {evaluated} images, {skipped} skipped\")\n",
    "        \n",
    "        # Generate reports\n",
    "        self.generate_evaluation_report()\n",
    "        self.plot_evaluation_results()\n",
    "        \n",
    "        return self.evaluation_results\n",
    "    \n",
    "    def generate_evaluation_report(self):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"No evaluation results to report\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.evaluation_results)\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            'total_images_evaluated': len(df),\n",
    "            'mean_iou': float(df['iou'].mean()),\n",
    "            'std_iou': float(df['iou'].std()),\n",
    "            'median_iou': float(df['iou'].median()),\n",
    "            'min_iou': float(df['iou'].min()),\n",
    "            'max_iou': float(df['iou'].max()),\n",
    "            'mean_dice': float(df['dice'].mean()),\n",
    "            'mean_pixel_accuracy': float(df['pixel_accuracy'].mean()),\n",
    "            'mean_f1_score': float(df['f1_score'].mean()),\n",
    "            'mean_precision': float(df['precision'].mean()),\n",
    "            'mean_recall': float(df['recall'].mean()),\n",
    "            'total_nuclei_predicted': int(df['n_predicted'].sum()),\n",
    "            'total_nuclei_ground_truth': int(df['n_true'].sum()),\n",
    "            'total_true_positives': int(df['true_positives'].sum()),\n",
    "            'total_false_positives': int(df['false_positives'].sum()),\n",
    "            'total_false_negatives': int(df['false_negatives'].sum()),\n",
    "            'evaluation_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save detailed results\n",
    "        df.to_csv(self.output_dir / 'detailed_evaluation_results.csv', index=False)\n",
    "        \n",
    "        # Save summary\n",
    "        with open(self.output_dir / 'evaluation_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SEGMENTATION EVALUATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Images Evaluated: {summary['total_images_evaluated']}\")\n",
    "        print(f\"\\nPixel-Level Metrics:\")\n",
    "        print(f\"  Mean IoU:            {summary['mean_iou']:.4f} (¬±{summary['std_iou']:.4f})\")\n",
    "        print(f\"  Mean Dice:           {summary['mean_dice']:.4f}\")\n",
    "        print(f\"  Mean Pixel Accuracy: {summary['mean_pixel_accuracy']:.4f}\")\n",
    "        print(f\"\\nObject-Level Metrics:\")\n",
    "        print(f\"  Mean Precision:      {summary['mean_precision']:.4f}\")\n",
    "        print(f\"  Mean Recall:         {summary['mean_recall']:.4f}\")\n",
    "        print(f\"  Mean F1 Score:       {summary['mean_f1_score']:.4f}\")\n",
    "        print(f\"\\nDetection Counts:\")\n",
    "        print(f\"  Ground Truth Nuclei: {summary['total_nuclei_ground_truth']:,}\")\n",
    "        print(f\"  Predicted Nuclei:    {summary['total_nuclei_predicted']:,}\")\n",
    "        print(f\"  True Positives:      {summary['total_true_positives']:,}\")\n",
    "        print(f\"  False Positives:     {summary['total_false_positives']:,}\")\n",
    "        print(f\"  False Negatives:     {summary['total_false_negatives']:,}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_evaluation_results(self):\n",
    "        \"\"\"Create comprehensive evaluation visualizations\"\"\"\n",
    "        df = pd.DataFrame(self.evaluation_results)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # IoU distribution\n",
    "        axes[0, 0].hist(df['iou'], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 0].axvline(df['iou'].mean(), color='red', linestyle='--', \n",
    "                          label=f'Mean: {df[\"iou\"].mean():.3f}')\n",
    "        axes[0, 0].set_xlabel('IoU Score')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('IoU Distribution')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Dice distribution\n",
    "        axes[0, 1].hist(df['dice'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "        axes[0, 1].axvline(df['dice'].mean(), color='red', linestyle='--',\n",
    "                          label=f'Mean: {df[\"dice\"].mean():.3f}')\n",
    "        axes[0, 1].set_xlabel('Dice Coefficient')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('Dice Coefficient Distribution')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # Precision vs Recall\n",
    "        axes[0, 2].scatter(df['recall'], df['precision'], alpha=0.6, s=50)\n",
    "        axes[0, 2].set_xlabel('Recall')\n",
    "        axes[0, 2].set_ylabel('Precision')\n",
    "        axes[0, 2].set_title('Precision vs Recall')\n",
    "        axes[0, 2].grid(alpha=0.3)\n",
    "        axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "        \n",
    "        # F1 Score distribution\n",
    "        axes[1, 0].hist(df['f1_score'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "        axes[1, 0].axvline(df['f1_score'].mean(), color='red', linestyle='--',\n",
    "                          label=f'Mean: {df[\"f1_score\"].mean():.3f}')\n",
    "        axes[1, 0].set_xlabel('F1 Score')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('F1 Score Distribution')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Predicted vs Ground Truth nuclei count\n",
    "        axes[1, 1].scatter(df['n_true'], df['n_predicted'], alpha=0.6, s=50)\n",
    "        axes[1, 1].set_xlabel('Ground Truth Count')\n",
    "        axes[1, 1].set_ylabel('Predicted Count')\n",
    "        axes[1, 1].set_title('Nuclei Count Comparison')\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "        # Add perfect prediction line\n",
    "        max_count = max(df['n_true'].max(), df['n_predicted'].max())\n",
    "        axes[1, 1].plot([0, max_count], [0, max_count], 'r--', alpha=0.5, label='Perfect')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        # Box plots of metrics\n",
    "        metrics_data = df[['iou', 'dice', 'pixel_accuracy', 'f1_score']]\n",
    "        axes[1, 2].boxplot([metrics_data[col] for col in metrics_data.columns],\n",
    "                          labels=['IoU', 'Dice', 'Pixel Acc', 'F1'])\n",
    "        axes[1, 2].set_ylabel('Score')\n",
    "        axes[1, 2].set_title('Metrics Overview')\n",
    "        axes[1, 2].grid(alpha=0.3, axis='y')\n",
    "        axes[1, 2].set_ylim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'evaluation_plots.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"‚úÖ Evaluation plots saved to {self.output_dir / 'evaluation_plots.png'}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Integration with your existing pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def add_evaluation_to_pipeline(pipeline, dataset_root='dataset'):\n",
    "    \"\"\"\n",
    "    Add evaluation capability to your existing NucleiSegmentationPipeline\n",
    "    \n",
    "    Usage:\n",
    "        pipeline = NucleiSegmentationPipeline()\n",
    "        pipeline.process_dataset('dataset', pattern='*.png')\n",
    "        \n",
    "        # Then evaluate\n",
    "        evaluator = add_evaluation_to_pipeline(pipeline, dataset_root='dataset')\n",
    "        results = evaluator.evaluate_dataset(\n",
    "            pred_mask_dir='results/plots',\n",
    "            true_mask_dir='dataset'\n",
    "        )\n",
    "    \"\"\"\n",
    "    evaluator = SegmentationEvaluator(output_dir=pipeline.output_dir / 'evaluation')\n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e137e661-1969-45df-81bd-edc1c40dccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting evaluation against ground truth...\n",
      "Found 670 predicted masks\n",
      "‚úì Evaluated 670/670\n",
      "‚úÖ Evaluation complete: 670 images, 0 skipped\n",
      "\n",
      "======================================================================\n",
      "SEGMENTATION EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Images Evaluated: 670\n",
      "\n",
      "Pixel-Level Metrics:\n",
      "  Mean IoU:            0.8504 (¬±0.1209)\n",
      "  Mean Dice:           0.9127\n",
      "  Mean Pixel Accuracy: 0.9795\n",
      "\n",
      "Object-Level Metrics:\n",
      "  Mean Precision:      0.9844\n",
      "  Mean Recall:         0.9381\n",
      "  Mean F1 Score:       0.9586\n",
      "\n",
      "Detection Counts:\n",
      "  Ground Truth Nuclei: 24,286\n",
      "  Predicted Nuclei:    23,122\n",
      "  True Positives:      22,857\n",
      "  False Positives:     265\n",
      "  False Negatives:     1,214\n",
      "======================================================================\n",
      "‚úÖ Evaluation plots saved to results/evaluation/evaluation_plots.png\n"
     ]
    }
   ],
   "source": [
    "evaluator = SegmentationEvaluator()\n",
    "\n",
    "results = evaluator.evaluate_dataset(pred_mask_dir='results/plots',true_mask_dir='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09627631-f33e-4f29-b592-8ae3ce8d5f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
